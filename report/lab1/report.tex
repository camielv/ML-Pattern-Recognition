\documentclass[a4paper]{article}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{url}
\usepackage{graphicx}

\title{k-Nearest-Neighbours\\\large Lab Session 1\\Machine Learning: Pattern Recognition\\Master Artificial Intelligence}

\author{Camiel Verschoor \\StudentID: 10017321\\UvAnetID: 6229298\\ \url{Verschoor@uva.nl} \and Steven Laan\\StudentID: 6036031\\UvAnetID: 6036031\\\url{S.Laan@uva.nl}}

\begin{document}

\maketitle

\section{Data Visualization}
\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\textwidth]{images/data_scatterplot.pdf}
\caption{Data visualization.}
\label{data}
\end{figure}
In figure \ref{data} the data in the training set, which consists of the two classes, A (red) and B (blue).

\newpage
\section{k-Nearest Neighbours}
A kNN classifier, with $k = 1$, is trained on the trainings data. The performance is evaluated on the test data. This resulted in the following confusion matrix:
\begin{center}
\begin{tabular}{ | l | l | l | }
\hline
 & \textbf{True} & \textbf{False}\\
\hline
\textbf{Positive} & 206 & 44\\
\hline
\textbf{Negative} & 36 & 214\\
\hline
\end{tabular}
\end{center}
From the confusion matrix the error rate is computed by the following formula:
\begin{equation}
1 - accuracy = 1 - \frac{tp + fp}{tp + tn + fp + fn}
\end{equation}
The error rate and other statistical measures of the classifier are presented in the table below:
\begin{center}
\begin{tabular}{ | l | l | }
\hline
\textbf{Accuracy} & 84.0\%\\
\hline
\textbf{Precision} & 82.4\%\\
\hline
\textbf{Recall} & 85.1\%\\
\hline
\textbf{F-measure} & 83.7\%\\
\hline
\textbf{Error rate} & 16.0\%\\
\hline
\end{tabular}
\end{center}
In figure \ref{error_rate} a graph is shown of the test error evolving as a function of k. Notice that test error increases when $k$ is growing, this is caused by overfitting on the training set. Furthermore, it can be noticed that the function is fluctuating, which is caused by the noise of the classifier.
\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\textwidth]{images/error_rate_1500_1.pdf}
\caption{Graph of the error rate with various K}
\label{error_rate}
\end{figure}\\
From this single experiment we cannot conclude a specific value of $k$, because it could be the case that the trainings data is not a suitable representation of the overall dataset. However, if a value has to be chosen we would consider using a statistical method like the P test to consider the answer. This answer would be around $k = 100$.

\section{Cross Validation}
The advantage of using a evaluation method like cross-validation and bootstrapping is that this method evaluates how well a hypothesis of the classifier performs on predicting new data.

A validation set is a portion of the dataset used to asses the performance of prediction or classification models that have been fit on a separate set, training data, of the dataset. The validation set is used as a more objective measure of performance of various models that have been fit on the training data as validating the performance with the training set is not likely to be a good guide to the performance of the models on new data.
\end{document}